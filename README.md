# Large Language Models for Computer-Aided Design: A Survey

This repo is constructed for collecting papers on state-of-the-art large language models as well as their applications on computer-aided design according to our survey paper——[_**Large Language Models for Computer-Aided Design: A Survey**_]().
# Overview
![Taxonomy](taxonomy.png)

# Catalogue
## [LLMs Taxonomy](#1)
### [Closed Source LLMs](#1.1)
- [The GPT Family](#1.1.1)
- [The PaLM Family](#1.1.2)
- [The Gemini Series](#1.1.3)
### [Publicly Available LLMs](#1.2)
  - [The LLaMA Family](#1.2.1)
  - [The DeepSeek Family](#1.2.2)
### [Others](#1.3)
## [CAD Application Taxonomy](#2)
### [Data Generation](#2.1)
### [CAD Code Generation](#2.2)
### [Parametric CAD Generation](#2.3)
### [Image Generation](#2.4)
### [Model Evaluation](#2.5)
### [Text Generation](#2.6)

<p id="1"></p >

## LLMs Taxonomy
<p id="1.1"></p >

### 1. Closed Source LLMs
<p id="1.1.1"></p >

#### 1.1 The GPT Family

[Language Models are Few-Shot Learners](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)

[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[Training language models to follow instructions with human feedback
](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)

[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

[GPT-4V(ision) System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)

[GPT-4o System Card](https://arxiv.org/abs/2410.21276)

[OpenAI o1 System Card](https://arxiv.org/abs/2412.16720)
<p id="1.1.2"></p >

#### 1.2 The PaLM Family

[PaLM: Scaling Language Modeling with Pathways](https://www.jmlr.org/papers/v24/22-1144.html)

[PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403)

[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)

[Toward expert-level medical question answering with large language models](https://www.nature.com/articles/s41591-024-03423-7)

[PaLM-E: An Embodied Multimodal Language Model](https://proceedings.mlr.press/v202/driess23a.html)

[Transcending Scaling Laws with 0.1% Extra Compute](https://openreview.net/forum?id=Cf6VhQFmhP)

[Scaling Instruction-Finetuned Language Models](https://www.jmlr.org/papers/v25/23-0870.html)
<p id="1.1.3"></p >

#### 1.3 The Gemini Series
[Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)

[Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/abs/2403.08295)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118)
<p id="1.2"></p >

### 2. Publicly Available LLMs
<p id="1.2.1"></p >

#### 2.1 The LLaMA Family
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)
<p id="1.2.2"></p >

#### 2.2 The DeepSeek Family
[DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)

[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://openreview.net/forum?id=EmUsC2FogT)

[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)

[DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)

[DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)
<p id="1.3"></p >

### 3. Others

[PanGu-?: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing](https://arxiv.org/abs/2303.10845)

[BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)
-------------------------------------------------------------------------


<p id="2"></p >






## CAD Application Taxonomy
<p id="2.1"></p >

### 1. Data Generation
<p id="2.2"></p >






### 2. CAD Code Generation
<p id="2.3"></p >





### 3. Parametric CAD Generation
<p id="2.4"></p >






### 4. Image Generation
<p id="2.5"></p >





### 5. Model Evaluation
<p id="2.6"></p >






### 6. Text Generation


