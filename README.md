# Large Language Models for Computer-Aided Design: A Survey

This repo is constructed for collecting papers on state-of-the-art large language models as well as their applications on computer-aided design according to our survey paper——[_**Large Language Models for Computer-Aided Design: A Survey**_]().
# Overview
![Taxonomy](taxonomy.png)

# Catalogue
## [LLMs Taxonomy](#1)
### [Closed Source LLMs](#1.1)
- [The GPT Family](#1.1.1)
- [The PaLM Family](#1.1.2)
- [The Gemini Series](#1.1.3)
### [Publicly Available LLMs](#1.2)
  - [The LLaMA Family](#1.2.1)
  - [The DeepSeek Family](#1.2.2)
### [Others](#1.3)
## [CAD Application Taxonomy](#2)
### [Data Generation](#2.1)
### [CAD Code Generation](#2.2)
### [Parametric CAD Generation](#2.3)
### [Image Generation](#2.4)
### [Model Evaluation](#2.5)
### [Text Generation](#2.6)

<p id="1"></p >

## LLMs Taxonomy
<p id="1.1"></p >

### 1. Closed Source LLMs
<p id="1.1.1"></p >

#### 1.1 The GPT Family

[Language Models are Few-Shot Learners](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)

[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)

[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)

[Training language models to follow instructions with human feedback
](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)

[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

[GPT-4V(ision) System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)

[GPT-4o System Card](https://arxiv.org/abs/2410.21276)

[OpenAI o1 System Card](https://arxiv.org/abs/2412.16720)
<p id="1.1.2"></p >

#### 1.2 The PaLM Family

[PaLM: Scaling Language Modeling with Pathways](https://www.jmlr.org/papers/v24/22-1144.html)

[PaLM 2 Technical Report](https://arxiv.org/abs/2305.10403)

[Large language models encode clinical knowledge](https://www.nature.com/articles/s41586-023-06291-2)

[Toward expert-level medical question answering with large language models](https://www.nature.com/articles/s41591-024-03423-7)

[PaLM-E: An Embodied Multimodal Language Model](https://proceedings.mlr.press/v202/driess23a.html)

[Transcending Scaling Laws with 0.1% Extra Compute](https://openreview.net/forum?id=Cf6VhQFmhP)

[Scaling Instruction-Finetuned Language Models](https://www.jmlr.org/papers/v25/23-0870.html)
<p id="1.1.3"></p >

#### 1.3 The Gemini Series
[Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
[Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)

[Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/abs/2403.08295)

[Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118)
<p id="1.2"></p >

### 2. Publicly Available LLMs
<p id="1.2.1"></p >

#### 2.1 The LLaMA Family
[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)
<p id="1.2.2"></p >

#### 2.2 The DeepSeek Family
[DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196)

[DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)

[DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://openreview.net/forum?id=EmUsC2FogT)

[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)

[DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)

[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)

[DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)
<p id="1.3"></p >

### 3. Others

[PanGu-{\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing](https://arxiv.org/abs/2303.10845)

[BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)

[The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)

-------------------------------------------------------------------------

[LIMA: Less Is More for Alignment](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html)

[Crosslingual Generalization through Multitask Finetuning](https://virtual2023.aclweb.org/paper_P283.html)

[QLoRA: Efficient Finetuning of Quantized LLMs](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html)

[Mistral 7B](https://arxiv.org/abs/2310.06825)

[Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)

[Gorilla: Large Language Model Connected with Massive APIs](https://proceedings.neurips.cc/paper_files/paper/2024/hash/e4c61f578ff07830f5c37378dd3ecb0d-Abstract-Conference.html)

[Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882)

[How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ec6413875e4ab08d7bc4d8e225263398-Abstract-Datasets_and_Benchmarks.html)

[Focused Transformer: Contrastive Training for Context Scaling](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8511d06d5590f4bda24d42087802cc81-Abstract-Conference.html)

[Qwen Technical Report](https://arxiv.org/abs/2309.16609)

[Visual Instruction Tuning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html)

[MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://openreview.net/forum?id=1tZbq88f27)

[InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)

[PandaGPT: One Model To Instruction-Follow Them All](https://aclanthology.org/2023.tllm-1.2/)

[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/abs/2406.12793)

[Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://proceedings.mlr.press/v202/biderman23a.html)

[CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309)

[StarCoder: may the source be with you!](https://openreview.net/forum?id=KoFOg41haE)

[Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)

[FLM-101B: An Open LLM and How to Train It with $100K Budget](https://arxiv.org/abs/2309.03852)

[Skywork: A More Open Bilingual Foundation Model](https://arxiv.org/abs/2310.19341)

[Llemma: An Open Language Model for Mathematics](https://openreview.net/forum?id=4WnqRR915j)

[WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions](https://openreview.net/forum?id=CfXh93NDgH)

[The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116)
<p id="2"></p >






## CAD Application Taxonomy
<p id="2.1"></p >

### 1. Data Generation
<p id="2.2"></p >






### 2. CAD Code Generation
<p id="2.3"></p >





### 3. Parametric CAD Generation
<p id="2.4"></p >






### 4. Image Generation
<p id="2.5"></p >





### 5. Model Evaluation
<p id="2.6"></p >






### 6. Text Generation


